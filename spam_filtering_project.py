# -*- coding: utf-8 -*-
"""Spam filtering Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1slVQlz-UFCkT0L2TYB28z0tIvlSgz6E5

We, Adarsh Kumar (DSTC-22/23-002) and Sourav Ghosh (DSTC-
22/23-016), hereby declare that, this report entitled  **“Spam or Ham? Iden-
tifying and Filtering Unwanted Emails using Bayesian analysis”** https://www.kaggle.com/datasets/venky73/spam-mails-dataset . submitted to Indian Statistical Institute Chennai requirement of Postgraduate
Diploma in Statistical Methods Analytics in Statistics in [Department
Name], is an original work carried out by me under the supervision of Dr. Sam-
pangi Raman and has not formed the basis for the award of any degree or
diploma, in this or any other institution or university. We have sincerely tried to
uphold academic ethics and honesty. Whenever a piece of external information or
statement or result is used then, that has been duly acknowledged and cited.
Chennai Adarsh Kumar, Sourav Ghosh
"""

! pip install -q kaggle

from google.colab import files
files.upload()

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d venky73/spam-mails-dataset

import zipfile
zip_ref = zipfile.ZipFile('/content/spam-mails-dataset.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()

import random
import re
import pandas as pd

df=pd.read_csv("/content/spam_ham_dataset.csv", header=None, names=['Label', 'Text','tag'])
df = df.drop('tag', axis=1)
df = df.drop(0)
df

df = df.iloc[1:]
df = df.reset_index(drop=True)
df

df.iloc[random.randint(0,len(df)+1),1]

"""**Data cleaning**

Data cleaning is the process of identifying and correcting or removing errors, inconsistencies, and inaccuracies in data. We do data cleaning for several reasons:


1. Improve Data Quality: Data cleaning helps to improve the quality of data by removing errors, inconsistencies, and inaccuracies that can affect the analysis and decision-making process.

2. Ensure Consistency: Data cleaning ensures that the data is consistent across different sources and formats. This is important when merging or aggregating data from different sources.

3. Remove Duplicates: Data cleaning helps to identify and remove duplicate records or observations. This is important for data analysis, as duplicates can skew the results and affect the accuracy of statistical models.

4. Handle Missing Data: Data cleaning helps to handle missing data by imputing values or removing observations with missing data. This is important as missing data can affect the accuracy of statistical models and data analysis.


5.
Ensure Data Validity: Data cleaning ensures that the data is valid by checking for outliers and extreme values. This is important for data analysis, as outliers can skew the results and affect the accuracy of statistical models.


6. Overall, data cleaning is important for ensuring that data is accurate, consistent, and valid, which is essential for effective data analysis and decision-making.

In order to classify a new message as spam or ham using our multinomial Naive Bayes algorithm, we need to calculate the probabilities of the message being spam or ham given the words in the message. To do this, we use a set of equations that involve calculating the probabilities of each word in the message given that the message is spam or ham. However, before we can perform these calculations, we need to clean the data to bring it into a format that is suitable for analysis.

The original format of our training and test sets consists of a column of messages, where each message is a string of words. To calculate the necessary probabilities, we need to transform this data into a format where each row corresponds to a single message and each column represents a unique word from the vocabulary. The vocabulary is the set of unique words that appear in all of the messages.

During this transformation, we replace the original SMS column with a series of new columns representing the unique words in the vocabulary. We also convert all the words to lowercase and remove any punctuation. This transformation allows us to easily extract information about the occurrence of each word in each message and use it to calculate the necessary probabilities for our Naive Bayes algorithm.

Bayes' Theorem is a mathematical formula that calculates the probability of an event based on prior knowledge of conditions that might be related to the event. In the context of text classification, we can use Bayes' Theorem to calculate the probability that a given message belongs to a particular class (such as spam or ham) based on the probability of observing certain words in the message.

In Python, the formula for Bayes' Theorem can be written as:

P(class|message) = P(message|class) * P(class) / P(message)

where:

P(class|message) is the probability that the message belongs to the given class, given the words in the message
P(message|class) is the probability of observing the words in the message, given the class
P(class) is the prior probability of the class (i.e., the overall probability that a message belongs to that class)
P(message) is the overall probability of observing the message (this factor is usually ignored since it is constant for all classes)
To use this formula in text classification, we first calculate the values of P(message|class) and P(class) for each class using our training data. Then, given a new message, we can calculate the value of P(class|message) for each class using the above formula and choose the class with the highest probability as the predicted class for the message.
"""

def remove_digits(sentence):
    pattern = r'\d+'
    return re.sub(pattern, '', sentence)

def remove_chars(string):
    remove = ['\r', '\n', ':', ',','.',';']
    return string.translate(str.maketrans('', '', ''.join(remove)))

def remove_special_chars(sentence):
    pattern = r'[^a-zA-Z0-9\s]'
    return re.sub(pattern, '', sentence)

def remove_long_spaces(sentence):
    words = sentence.split()                        # split the sentence into words
    clean_words = [word for word in words if word]  # remove empty words
    return " ".join(clean_words)

def convert_labels(labels):
    new_labels = []
    for label in labels:
        if label == 'spam':
            new_labels.append(2)
        else:
            new_labels.append(0)
    return new_labels

def remove_single_letters(sentence):
    words = sentence.split()                   # split the sentence into words
    clean_words = [word for word in words if len(word) > 1]  # remove single-letter words
    return " ".join(clean_words)              # join the remaining words into a sentence



df["Label"] = df["Label"].apply(lambda x: 1 if "spam" in x.lower() else 0)
df["Text"]=df["Text"].apply(remove_digits)
df["Text"]=df["Text"].apply(remove_long_spaces)
df["Text"]=df["Text"].apply(remove_chars)
df["Text"]=df["Text"].apply(remove_special_chars)
df['Text']=df['Text'].str.lower()
df['Text']=df['Text'].apply(remove_single_letters)
df.sample(5)

df.iloc[random.randint(0,len(df)+1),1]

df.groupby('Label').describe()

df['Label'].value_counts(normalize=True)

"""**Data visualization**


Data visualization is an important step in the process of filtering spam data because it allows us to better understand the data and identify patterns and trends.

Spam filtering involves classifying incoming messages as either spam or non-spam (also known as "ham"). Data visualization can help us to visualize the features that distinguish spam messages from non-spam messages, such as the frequency of certain words or phrases, the length of the message, and the presence of certain types of attachments or links.

By visualizing these features using graphs, charts, and other visualizations, we can gain insights into the characteristics of spam messages and identify features that are useful for classification. For example, we might create a histogram of the word frequencies in the spam messages and compare it to a histogram of the word frequencies in the non-spam messages. This could help us to identify words or phrases that are more common in spam messages than in non-spam messages.

Data visualization also helps to communicate the results of our analysis to others. By creating visualizations that effectively communicate our findings, we can share our insights with stakeholders and make data-driven decisions about how to improve our spam filtering algorithms.
"""

import matplotlib.pyplot as plt

# Count the number of spam and ham messages
spam_count = df[df['Label'] == 1]['Label'].count()
ham_count = df[df['Label'] == 0]['Label'].count()

# Set the figure size
fig = plt.figure(figsize=(10, 6))  # Adjust the width and height as desired

# Create a bar chart of the message counts
labels = ['Spam', 'Ham']
counts = [spam_count, ham_count]
plt.bar(labels, counts)
plt.title('Spam/Ham Message Counts')
plt.xlabel('Message Type')
plt.ylabel('Count')
plt.show()

from sklearn.model_selection import train_test_split
# Splitting the data into training and testing sets
X_train, X_test = train_test_split(df , test_size=0.4, random_state=42)

X_test

X_train

import math
import re
from collections import defaultdict
import pandas as pd

class NaiveBayesClassifier:
    def __init__(self):
        self.word_counts = defaultdict(lambda: [0, 0])  # [spam_count, ham_count]
        self.total_spam = 0
        self.total_ham = 0
        self.spam_prior = 0.0
        self.ham_prior = 0.0

    def train(self, training_data):
        for _, row in training_data.iterrows():
            text, label = row['Text'], row['Label']
            words = self.preprocess_text(text)
            for word in words:
                self.word_counts[word][label] += 1
                if label == 1:
                    self.total_spam += 1
                else:
                    self.total_ham += 1

        total_messages = self.total_spam + self.total_ham
        self.spam_prior = self.total_spam / total_messages
        self.ham_prior = self.total_ham / total_messages

    def classify(self, text):
        words = self.preprocess_text(text)
        spam_prob = math.log(self.spam_prior)
        ham_prob = math.log(self.ham_prior)

        for word in words:
            word_data = self.word_counts[word]
            spam_prob += math.log((word_data[1] + 1) / (self.total_spam + len(self.word_counts)))
            ham_prob += math.log((word_data[0] + 1) / (self.total_ham + len(self.word_counts)))

        return spam_prob > ham_prob

    @staticmethod
    def preprocess_text(text):
        # Preprocess the text by converting to lowercase and extracting words
        words = re.findall(r'\w+', text.lower())
        return words

"""This code defines a class called NaiveBayesClassifier. The __init__ method is the class constructor where the initial values of various variables are set. The word_counts variable is a dictionary that stores the count of words in spam and ham messages. total_spam and total_ham keep track of the total number of spam and ham messages. spam_prior and ham_prior represent the prior probabilities of spam and ham messages, respectively.


The train method takes in the training data, which is expected to be a DataFrame with columns 'Text' and 'Label'. It iterates over each row of the training data, preprocesses the text using the preprocess_text method, and updates the word counts based on the label (spam or ham). It also keeps track of the total number of spam and ham messages. Finally, it calculates and stores the prior probabilities of spam and ham messages.

The classify method takes a text as input, preprocesses it using the preprocess_text method, and then calculates the probability of the text being spam or ham using the Naive Bayes formula. The logarithm of the probabilities is used to avoid numerical underflow. The method iterates over each word in the preprocessed text and calculates the conditional probabilities based on the word counts. Finally, it returns a boolean value indicating whether the text is classified as spam (True) or ham (False).

The preprocess_text method takes a text as input, converts it to lowercase, and extracts individual words using regular expression matching (re.findall). The extracted words are returned as a list.
"""

# Create a NaiveBayesClassifier instance
classifier = NaiveBayesClassifier()
# Train the classifier
classifier.train(X_train)

def calculate_accuracy(model, test_data):
    total_examples = len(test_data)
    correct_predictions = 0

    for _, row in test_data.iterrows():
        text, true_label = row['Text'], row['Label']
        predicted_label = model.classify(text)

        if predicted_label == true_label:
            correct_predictions += 1

    accuracy = correct_predictions / total_examples
    return accuracy

import matplotlib.pyplot as plt

thresholds = [0.1, 0.2, 0.3, 0.4, 0.5,0.6,0.7,0.8,0.9,.99]  # Example thresholds
accuracies = []

for threshold in thresholds:
    test_size = threshold  # Calculate test size based on the threshold
    X_train, X_test = train_test_split(df, test_size=test_size, random_state=42)
    classifier.train(X_train)
    accuracy = calculate_accuracy(classifier, X_test)
    accuracies.append(accuracy)

# Plot the accuracy graph
plt.plot(thresholds, accuracies, marker='o')
plt.xlabel('Threshold')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Threshold')
plt.grid(True)
plt.show()

# Assuming you have trained the model and have a test dataset named 'test_data'
accuracy = calculate_accuracy(classifier, X_test)
print("Accuracy:", accuracy)

# Test the classifier
email = "offer!offer!offer"
is_spam = classifier.classify(email)

if is_spam:
    print(f"The email '{email}' is classified as spam.")
else:
    print(f"The email '{email}' is classified as ham.")

# Test the classifier
email = "”offer!offer!offer"
is_spam = classifier.classify(email)

if is_spam:
    print(f"The email '{email}' is classified as spam.")
else:
    print(f"The email '{email}' is classified as ham.")

#here we are using the threshold value 0.4 i.e., the split of the  dataset is 60%(trainset),40%(testset) which satisfies the spliting condition of dataset (Data should be split so that data sets can have a high amount of training data ) as well as it is

